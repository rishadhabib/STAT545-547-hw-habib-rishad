---
title: "hw10_jcrscrape"
author: "RH"
date: "December 7, 2017"
output:   github_document: 
    toc: true
    toc_depth: 5
---

```{r setup, include=FALSE}
## Loading libraries
library(tidyverse)
library(magrittr)
library(purrr)
library(stringr)
library(glue)

library(xml2)
library(rvest)
library(tidytext)
library(knitr)

```

## set link
##### this is set as a dynamic link so that all issues from a volume can be obtained

```{r}
link <- function(volume = 44, issue = 1){
	
	# get all combinations of the end of the link
	link_end <- paste(volume, issue, sep="/")
	
	# glue them all to the first bit
	lapply(link_end, function(link_end) 
		glue("https://academic.oup.com/jcr/issue/{link_end}"))
}	

# test it out
volume <- 43
issue <- 3
article_link <- as.character(link(volume, issue))
article_link
```

### Now we can read in data from that link

```{r}
article_title <- read_html(article_link)
```



## Creating the dataframe

```{r}
jcr <- data_frame(volume = volume,
				  issue = issue, 
				  
                  # let's get the title of the article 
                  title = article_title %>%
                    html_nodes(css = ".customLink.item-title") %>%    
                    html_text(trim = TRUE),  
                  
                  # and the citation
                  citation = article_title %>%
                    html_nodes(css = ".ww-citation-primary") %>%   
                    html_text(), 
                  
                  # and the link to the main article where we can find the abstract
                  article = article_title %>%
                    html_nodes(css = ".customLink.item-title a") %>%
                    html_attr("href"), 
                  
                  link = glue("https://academic.oup.com{article}"))
```


#### View the dataframe

```{r}
View(jcr)
```

#### Function to extract the abstract from the link
```{r}
get_abstract <- function(link){         # this link points to a link that is extracted
	
	read_html(link) %>%                       # this is the link
		html_nodes(css = ".abstract") %>%
		html_text()
}

# check to see if function works with a sample article link
get_abstract("https://academic.oup.com/jcr/article/44/3/477/2939534")
```



#### add to our jcr dataframe

```{r}
jcr %<>%
	mutate(abstract = sapply(jcr$link, get_abstract))
```


#### add the author names
```{r}
authors <- article_title %>%
	html_nodes(css = ".al-authors-list") %>%    # use .title for the class = title
	html_text(trim = TRUE)

authors <- c(authors, rep("NA", nrow(jcr)-length(authors)))

jcr %<>%
	mutate(authors = authors, abstract = as.character(abstract))

```

#### View our final dataframe
```{r}
View(jcr)
```


#### Lastly let's write the file with the volume and issue in the csv file
```{r}
write.csv(as.data.frame(jcr), file = paste0("jcr", volume, issue, ".csv"), row.names = FALSE)
```



#### Sentiment analysis

```{r}

afinn <- get_sentiments(("afinn"))
jcr %>%
    unnest_tokens (word, abstract) %>%
    anti_join(stop_words, by = "word") %>% #remove dull words
    inner_join(afinn, by = "word") %>% #stitch scores
    group_by(title) %>% #and for each article title
    summarise(Length = n(), #do the math
              Score = sum(score)/Length) %>%
    arrange(-Score) %>%
	kable()
```

So we get the most positive article about compromise and the most negative article about debt.





